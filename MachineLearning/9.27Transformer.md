# input embedding

## 独热编码

一句话有N个词，就会生成NXN矩阵，每个字对应一个位置的1，其余位置都为0

<img src="https://raw.githubusercontent.com/V0ya93r/PicBed/main/img/image-20240927102214483.png" alt="image-20240927102214483" style="zoom:33%;" />

优点：计算简单 			 缺点：过于稀疏占用资源，==无法体现词与词之间的关系==



## word embedding

通过矩阵乘法降维聚合（宋浩公式）：10x10矩阵 * 10x5矩阵 ——>10x5矩阵，降维一半。

设计一个可学习权重矩阵W，W是5x10矩阵，通过点乘10x10矩阵可降维到5x10的矩阵。

举例：queen（皇后）= king（国王）- man（男人）+ woman（女人）

​	walked（过去式）= walking（进行时）- swimming（进行时）+ swam（过去式）

​	“北京”是“中国”的首都，“巴黎”是“法国”的首都，那么向量：|中国|-|北京|=|法国|-|巴黎|

# Position Encoding

